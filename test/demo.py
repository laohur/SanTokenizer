# -*- coding: utf-8 -*-


from timeit import repeat
import sys
from UnicodeTokenizer import UnicodeTokenizer


def demo(doc):
    head = [
        "chars num"
        "tokens num",
        "sentence",
        "tokens",
    ]
    result = [head]
    for line in doc:
        tokens = uToker.tokenize(line)
        row = [len(line),len(tokens), line.replace("\n", "Ã˜"), "â–".join(tokens).replace("\n", "Ã˜")]
        result.append(row)
    print(result)
    with open("result.tsv", "w") as f:
        for row in result:
            line = "\t".join(str(x) for x in row)
            f.write(line + "\n")


if __name__ == "__main__":
    uToker = UnicodeTokenizer()

    doc = [
        " ï¡¿                                                         ",
        "Ù†Ù†ØµØ­Ùƒ Ø£Ù† ØªØ³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø© Ø³Ø± ÙØ±ÙŠØ¯Ø© Ù„Ø§ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø¹Ù„Ù‰ Ø£ÙŠ Ù…ÙˆÙ‚Ø¹ Ø´Ø¨ÙƒÙŠ Ø¢Ø®Ø±.",
        "à¹à¸à¹Šà¸ªà¸¡à¸µà¸ªà¸à¸¸à¸¥ à¸«à¸£à¸·à¸­ à¹à¸à¹Šà¸ªà¸¡à¸µà¸•à¸£à¸°à¸à¸¹à¸¥ (à¹ƒà¸™à¸­à¸”à¸µà¸•à¹€à¸£à¸µà¸¢à¸à¸§à¹ˆà¸² à¹à¸à¹Šà¸ªà¹€à¸‰à¸·à¹ˆà¸­à¸¢ à¸«à¸£à¸·à¸­à¸šà¸²à¸‡à¸„à¸£à¸±à¹‰à¸‡à¹ƒà¸Šà¹‰à¸Šà¸·à¹ˆà¸­à¸§à¹ˆà¸² aerogens) à¹€à¸›à¹‡à¸™à¸à¸¥à¸¸à¹ˆà¸¡à¸‚à¸­à¸‡à¸˜à¸²à¸•à¸¸à¸—à¸²à¸‡à¹€à¸„à¸¡à¸µà¸—à¸µà¹ˆà¸¡à¸µà¸ªà¸¡à¸šà¸±à¸•à¸´à¸„à¸¥à¹‰à¸²à¸¢à¸à¸±à¸™ à¸ à¸²à¸¢à¹ƒà¸•à¹‰à¸ à¸²à¸§à¸°à¸¡à¸²à¸•à¸£à¸à¸²à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸­à¸¸à¸“à¸«à¸ à¸¹à¸¡à¸´à¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¸”à¸±à¸™à¸˜à¸²à¸•à¸¸à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰à¸•à¹ˆà¸²à¸‡à¹„à¸¡à¹ˆà¸¡à¸µà¸à¸¥à¸´à¹ˆà¸™ à¹„à¸¡à¹ˆà¸¡à¸µà¸ªà¸µ à¹€à¸›à¹‡à¸™à¹à¸à¹Šà¸ªà¸­à¸°à¸•à¸­à¸¡à¹€à¸”à¸µà¹ˆà¸¢à¸§à¸‹à¸¶à¹ˆà¸‡à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¸§à¸²à¸¡à¸§à¹ˆà¸­à¸‡à¹„à¸§à¸•à¹ˆà¸­à¸›à¸à¸´à¸à¸£à¸´à¸¢à¸²à¹€à¸„à¸¡à¸µ à¹à¸à¹Šà¸ªà¸¡à¸µà¸ªà¸à¸¸à¸¥à¸—à¸µà¹ˆà¹€à¸à¸´à¸”à¹ƒà¸™à¸˜à¸£à¸£à¸¡à¸Šà¸²à¸•à¸´à¸—à¸±à¹‰à¸‡à¸«à¸à¸˜à¸²à¸•à¸¸ à¹„à¸”à¹‰à¹à¸à¹ˆ à¸®à¸µà¹€à¸¥à¸µà¸¢à¸¡ (He), à¸™à¸µà¸­à¸­à¸™ (Ne), à¸­à¸²à¸£à¹Œà¸à¸­à¸™ (Ar), à¸„à¸£à¸´à¸›à¸—à¸­à¸™ (Kr), à¸‹à¸µà¸™à¸­à¸™ (Xe) (à¹ƒà¸™à¸ à¸²à¸žà¸•à¸²à¸¡à¸¥à¸³à¸”à¸±à¸š) à¹à¸¥à¸°à¹€à¸£à¸”à¸­à¸™ (Rn)  à¹‚à¸­à¸à¸²à¹€à¸™à¸ªà¸‹à¸­à¸™ (Og) à¹€à¸›à¹‡à¸™à¸˜à¸²à¸•à¸¸à¸ªà¸±à¸‡à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸¡à¸µà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¸à¸±à¸¡à¸¡à¸±à¸™à¸•à¸£à¸±à¸‡à¸ªà¸µà¸ªà¸¹à¸‡à¸¡à¸²à¸ à¹à¸¡à¹‰à¸§à¹ˆà¸² IUPAC à¸ˆà¸±à¸”à¹‚à¸­à¸à¸²à¹€à¸™à¸ªà¸‹à¸­à¸™à¹€à¸›à¹‡à¸™à¹à¸à¹Šà¸ªà¸¡à¸µà¸ªà¸à¸¸à¸¥à¸«à¸£à¸·à¸­à¸«à¸¡à¸¹à¹ˆà¸—à¸µà¹ˆ 18 à¸¡à¸±à¸™à¸­à¸²à¸ˆà¹„à¸¡à¹ˆà¹€à¸‰à¸·à¹ˆà¸­à¸¢à¸—à¸²à¸‡à¹€à¸„à¸¡à¸µà¹€à¸«à¸¡à¸·à¸­à¸™à¸˜à¸²à¸•à¸¸à¸­à¸·à¹ˆà¸™à¹ƒà¸™à¸«à¸¡à¸¹à¹ˆà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ à¹à¸¥à¸°à¸–à¸¹à¸à¸—à¸³à¸™à¸²à¸¢à¸§à¹ˆà¸²à¸ˆà¸°à¸ªà¸¥à¸²à¸¢à¹à¸¥à¸°à¹à¸œà¹ˆà¸à¸±à¸¡à¸¡à¸±à¸™à¸•à¸£à¸±à¸‡à¸ªà¸µà¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸à¸›à¸£à¸²à¸à¸à¸à¸²à¸£à¸“à¹Œà¸ªà¸±à¸¡à¸žà¸±à¸—à¸˜à¹Œ à¹€à¸™à¸·à¹ˆà¸­à¸‡à¸ˆà¸²à¸à¸„à¸£à¸¶à¹ˆà¸‡à¸Šà¸µà¸§à¸´à¸•à¸—à¸µà¹ˆà¸ªà¸±à¹‰à¸™à¹€à¸žà¸µà¸¢à¸‡ 0.7 à¹„à¸¡à¹‚à¸„à¸£à¸§à¸´à¸™à¸²à¸—à¸µà¸‚à¸­à¸‡à¹„à¸­à¹‚à¸‹à¹‚à¸—à¸›à¸•à¸±à¸§à¹€à¸”à¸µà¸¢à¸§ à¸—à¸³à¹ƒà¸«à¹‰à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´à¸—à¸²à¸‡à¹€à¸„à¸¡à¸µà¸‚à¸­à¸‡à¹‚à¸­à¸à¸²à¹€à¸™à¸ªà¸‹à¸­à¸™à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µà¸à¸²à¸£à¸¨à¸¶à¸à¸©à¸²à¸¡à¸²à¸à¸™à¸±à¸     é¦–å…ˆ8.88è®¾ç½® stã€‚art_new_word=True å’Œ output=[aÃ§aÃ­]ï¼Œoutput å°±æ˜¯æœ€ç»ˆï¡¿î´°ï¿½ no such name",
        "á  á ©á ¬á Žá   á ³á ¤á ®á ³á  á ³á ¤â€¯á ¶á ¢á ¨ á ®á £á ©á ­á ¤á ¯ á ¬á ¡á ¯á ¡á ¨â€¯á ¤ á ¬á ¢á ´á ¢á ¶á ¡á ¯â€¯á ¤á ¨ á ªá ¡á ¯á ¡á ³á ­á ¡á ®á µá ¢ á ®á ¢á ¨á £ á ³á ¤á °á ¬á  á ¢ á °á ¡á ³á ¤á ªâ€¯á ¤á ¨ á ­á ¤á £á ¯ á  á ­á ¤á ¯á ­á Žá  â€¯á ¨á ¢ á  á ©á ¬á Žá   á ³á ¤á ®á ³á  á ³á ¤â€¯á ¶á ¢á ¨ á ®á £á ©á ­á ¤á ¯ á ¬á ¡á ¯á ¡á ¨â€¯á ¤ á ¬á ¢á ´á ¢á ¶á ¡á ¯â€¯á ³á ¤ á ³á  á ­á  á ¯á ³á ¤á ­á ¤á ¯á ¤á ¨ á µá £á ¬á ¢á ¶á  á ­á °á  á ¨ á ºá £á £á ·á °á ¸á  á ¢á · á ªá £á ¯á ¤á ¨á Žá  á ƒ á ²á ¤á ° á ºá £á £á ·á °á ¸á  á ¢á ·â€¯á ²á ¤â€¯á ªá  á ¨ á °á ¤á ·á ¤á ­á ´á ¢â€¯á ¶á ¢á ¨ á °á ¤á ·á ¬á ¤ á ¤á ·á ®á Žá   á ªá  á ¬á Žá  â€¯á ¶á ¢ á ³á  á ³á  á ¬á ¤á ‚ á ¬á ¢á ´á ¢á ¶á ¡á ¯â€¯á ¤á ¨ á ­á ¤á £á ¯á ³á  á ¯á ­á Žá   á ¬á ¦á ´á ¢á ·á ³á ¡á ¯á ­á ¡â€¯á ¶á ¢ á ³á  á ­á ¤á ¯á µá ¤ á ¬á ¢á ´á ¢á ¶á ¡á ¯á ¯á ¡á ­á ¡â€¯á ¶á ¢á ¨ á ¶á  á ªá ¤á ´á  â€¯á ªá  á ¨ á ªá ¦á ·á ¢á ¨á µá ¢á ¬á ¦á ¯á µá ¤á ‚ á ®á ¡á ³á ¡á ¯á ­á ¡â€¯á ¶á ¢á ¨ á ¥á ·á ­á ¡á ³á ­á ¡á ¯â€¯á ¢á ¶á  á · á ³á  á ®á µá ¢á ¨ á µá ¢á ­á  á ¨ á °á ¤á ·á ­á  á ¬á ¤ á ´á ¢á ¨á  á · á ´á ¢á ¨á °á  á ­á Žá  â€¯á ªá  á ¨ á ³á ¡á ­á ¡á ­á °á ¢á ¯á ¡á ­á ¦á ¯á ¬á ¦ á ¬á ¡á °á ¡á ¨ á µá £á ·á ¢á ¯á ­á Žá  â€¯á ²á  á ¢á ƒ á ²á ¤á ° á ¬á ¥á ®á ¦á ¨â€¯á ¤ á ¬á ¤á ªá ¢â€¯á ¶á ¢á ¨ á ¬á ¡á ª á ´á ¢á ¨á  á ·á ‚ á ®á ¡á ³á ¡á ¯á ­á ¡ á ´á ¢á ³á  á ªá ¤á ·á ¢â€¯á ¶á ¢á ¨ á ®á ¡á ¬á ¦á °â€¯á  á ´á   á ªá £á ¯á µá ¤ á ªá ¡á ¯á ¡á ³á ­á ¡á ®á µá ¢â€¯á ¶á ¢á ¨ á ³á ¤á ®á ³á   á  á °á  á ­á ¤á ³á  á ¯ á £á ·á ¤á °á ¢á µá ¤ á ªá  á ¢á ­á Žá  â€¯á ¨á ¢ á ¯á  á ªá ³á  á ¢á ƒ á  á ©á ¬á  á ·á ¤á ¯â€¯á ¶á ¢á ¨ á ¬á  á ¨á ³á ¤á ­á ¤á ¯á ¤á ­á °á  á ¨ á ²á   á ªá ¦á ¬á ¦á ¨ á ¢á ¯á ¡á ­á ¦á ¬á ¡á ¨ á °á  á ¨á  á ¯ á µá ¥á ªá ¯á ¡á ¯á ­á ¡ á ¥á ­á ¬á ¦â€¯á ¶á ¢ á ¬á ¦á °á ¡á µá ¤ á ªá  á ¢á ¨á Žá  á ƒ    çš„è¾“å‡ºà¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸žà¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°íƒ‘ìŠ¹ ìˆ˜ì†í•´ì•¼pneumonoultramicroscopicsilicovolcanoconiosis",
        "å››è‰²å®šç†æ˜¯ä¸€ä¸ªè‘—åçš„æ•°å­¦å®šç†ï¼šå¦‚æžœåœ¨å¹³é¢ä¸Šåˆ’å‡ºä¸€äº›é‚»æŽ¥çš„æœ‰é™åŒºåŸŸï¼Œé‚£ä¹ˆå¯ä»¥ç”¨å››ç§é¢œè‰²æ¥ç»™è¿™äº›åŒºåŸŸæŸ“è‰²ï¼Œä½¿å¾—æ¯ä¸¤ä¸ªé‚»æŽ¥åŒºåŸŸæŸ“çš„é¢œè‰²éƒ½ä¸ä¸€æ ·ã€‚è¢«ç§°ä¸ºé‚»æŽ¥çš„ä¸¤ä¸ªåŒºåŸŸæ˜¯æŒ‡å®ƒä»¬æœ‰ä¸€æ®µå…¬å…±çš„è¾¹ç•Œï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªå…¬å…±çš„äº¤ç‚¹ã€‚å››è‰²é—®é¢˜æœ€æ—©æ˜¯ç”±è‹±å›½æ•°å­¦å®¶æ³•å…°è¥¿æ–¯Â·å¤å¾·é‡Œåœ¨1852å¹´æå‡ºçš„ã€‚äººä»¬å‘çŽ°ï¼Œè¦è¯æ˜Žå®½æ¾ä¸€ç‚¹çš„â€œäº”è‰²å®šç†â€å¾ˆå®¹æ˜“ï¼Œä½†å››è‰²é—®é¢˜å´å‡ºäººæ„æ–™åœ°å¼‚å¸¸å›°éš¾ã€‚1976å¹´ï¼Œæ•°å­¦å®¶å‡¯å°¼æ–¯Â·é˜¿ä½©å°”å’Œæ²ƒå¤«å†ˆÂ·å“ˆè‚¯å€ŸåŠ©ç”µå­è®¡ç®—æœºé¦–æ¬¡å¾—åˆ°ä¸€ä¸ªå®Œå…¨çš„è¯æ˜Žï¼Œå››è‰²é—®é¢˜ä¹Ÿç»ˆäºŽæˆä¸ºå››è‰²å®šç†ã€‚è¿™æ˜¯é¦–ä¸ªä¸»è¦å€ŸåŠ©è®¡ç®—æœºè¯æ˜Žçš„å®šç†ã€‚",
        """est ð—´‚ð—¹­ð˜œ¶ð—´²ð—‚§, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par Â« pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹). """,
        "èœ‚èœœã¨ã¯ãƒŸãƒ„ãƒãƒãŒèŠ±ã®èœœã‚’æŽ¡é›†ã—ã€å·£ã®ä¸­ã§åŠ å·¥ã€è²¯è”µã—ãŸã‚‚ã®ã‚’ã„ã†ã€‚ç´„8å‰²ã®ç³–åˆ†ã¨ç´„2å‰²ã®æ°´åˆ†ã«ã‚ˆã£ã¦æ§‹æˆã•ã‚Œã€ãƒ“ã‚¿ãƒŸãƒ³ã¨ãƒŸãƒãƒ©ãƒ«é¡žãªã©ã®æ „é¤Šç´ ã‚’ã‚ãšã‹ã«å«ã‚€ã€‚å‘³ã‚„è‰²ã¯èœœæºæ¤ç‰©ã«ã‚ˆã£ã¦æ§˜ã€…ã§ã‚ã‚‹ã€‚  æœ¬æ¥ã¯ãƒŸãƒ„ãƒãƒã®é£Ÿæ–™ã§ã‚ã‚‹ãŒã€ã—ã°ã—ã°ä»–ã®ç”Ÿç‰©ãŒæŽ¡é›†ã—ã¦é£Ÿæ–™ã¨ã—ã¦ã„ã‚‹ã€‚äººé¡žã‚‚ã€Œèœ‚èœœã®æ­´å²ã¯äººé¡žã®æ­´å²ã€ã¨ã„ã†ã“ã¨ã‚ã–ãŒã‚ã‚‹ã‚ˆã†ã«ã€å¤æ¥ã€é£Ÿç”¨ã€è–¬ç”¨ãªã©æ§˜ã€…ãªç”¨é€”ã«ç”¨ã„ã¦ã„ã‚‹ã€‚äººé¡žã¯åˆã‚ã€é‡Žç”Ÿã®ãƒŸãƒ„ãƒãƒã®å·£ã‹ã‚‰èœ‚èœœã‚’æŽ¡é›†ã—ã¦ã„ãŸãŒã€ã‚„ãŒã¦ãƒŸãƒ„ãƒãƒã‚’é£¼è‚²ã—ã¦æŽ¡é›†ã™ã‚‹ã“ã¨ï¼ˆé¤Šèœ‚ï¼‰ã‚’èº«ã«ä»˜ã‘ãŸã€‚äººé¡žã«ã‚ˆã‚‹èœ‚èœœã®ç”Ÿç”£é‡ã¯ã€ä¸–ç•Œå…¨ä½“ã§å¹´é–“ç´„120ä¸‡tã¨æŽ¨å®šã•ã‚Œã‚‹ã€‚",
        "[2]: Different from the embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. For example, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 documents to get the final top-3 results.    ",
        "ì˜¤ìŠ¤íŠ¸ë ˆì¼ë¦¬ì•„ì²­ê°œêµ¬ë¦¬(Australian green tree frog)ëŠ” ì˜¤ìŠ¤íŠ¸ë ˆì¼ë¦¬ì•„ì™€ ë‰´ê¸°ë‹ˆ ì›ì‚°ì˜ ì²­ê°œêµ¬ë¦¬ê³¼ì— ì†í•œ ì²­ê°œêµ¬ë¦¬ì˜ ì¼ì¢…ì´ë‹¤. ë¯¸êµ­ê³¼ ë‰´ì§ˆëžœë“œì—ë„ ì™¸ëž˜ì¢…ìœ¼ë¡œ í˜ëŸ¬ë“¤ì–´ê°”ëŠ”ë°, ë‰´ì§ˆëžœë“œì—ì„œëŠ” ë©¸ì¢…ëœ ê²ƒìœ¼ë¡œ ì—¬ê²¨ì§„ë‹¤. í•™ëª… ëª…ëª…ìž ì¡´ í™”ì´íŠ¸ì˜ ì´ë¦„ì„ ë”° í™”ì´íŠ¸ì²­ê°œêµ¬ë¦¬(White's tree frog), íŠ¹ìœ ì˜ ìƒê¹€ìƒˆì—ì„œ ìœ ëž˜í•œ ì‹œë¬´ë£©ì²­ê°œêµ¬ë¦¬(dumpy tree frog) ë“±ì˜ ë³„ëª…ì´ ìžˆìœ¼ë©°, í•™ëª…ì€ ë¦¬í† ë¦¬ì•„ ì¹´ì—ë£°ë ˆì•„(Litoria caerulea)ì´ë‹¤. í˜•íƒœí•™ì ìœ¼ë¡œ ì˜¤ìŠ¤íŠ¸ë ˆì¼ë¦¬ì•„ì²­ê°œêµ¬ë¦¬ì†ì˜ ë‹¤ë¥¸ ì²­ê°œêµ¬ë¦¬, íŠ¹ížˆ ì˜ˆìœì²­ê°œêµ¬ë¦¬ì™€ ì™•ì²­ê°œêµ¬ë¦¬ì™€ ë§Žì´ ë‹®ì•˜ë‹¤. ",
        "Ð­Ð»Ð¾Ð¹ Ð±Ñ‹Ð»Ð° Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ð°Ð½Ð° ÑÑ‚ÑƒÐ´Ð¸ÐµÐ¹ Guerrilla Games, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ñ…Ð¾Ñ‚ÐµÐ»Ð° ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÐ¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¸ Ð³ÐµÑ€Ð¾Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ Ð¶ÐµÐ½ÑÐºÐ¾Ð³Ð¾ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð° Ð² Ñ€Ð°Ð¼ÐºÐ°Ñ… ÑÐ²Ð¾ÐµÐ¹ ÑÐ¾Ð²ÐµÑ€ÑˆÐµÐ½Ð½Ð¾ Ð½Ð¾Ð²Ð¾Ð¹ Ð¸Ð³Ñ€Ñ‹. Ð›Ð¸Ñ†Ð¾ Ð³ÐµÑ€Ð¾Ð¸Ð½Ð¸ Ð±Ñ‹Ð»Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¾ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð²Ð½ÐµÑˆÐ½Ð¾ÑÑ‚Ð¸ Ð½Ð¸Ð´ÐµÑ€Ð»Ð°Ð½Ð´ÑÐºÐ¾Ð¹ Ð°ÐºÑ‚Ñ€Ð¸ÑÑ‹ Ð¥Ð°Ð½Ð½Ñ‹ Ð¥ÑƒÐºÑÑ‚Ñ€Ñ‹; Ñ€Ð¾Ð»ÑŒ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð¶Ð° Ð¿Ñ€Ð¸ Ð¾Ð·Ð²ÑƒÑ‡Ð¸Ð²Ð°Ð½Ð¸Ð¸ Ð½Ð° Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ Ð¸ Ð² ÑÑ†ÐµÐ½Ð°Ñ… Ð·Ð°Ñ…Ð²Ð°Ñ‚Ð° Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð¸ÑÐ¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð°Ð¼ÐµÑ€Ð¸ÐºÐ°Ð½ÑÐºÐ°Ñ Ð°ÐºÑ‚Ñ€Ð¸ÑÐ° Ð­ÑˆÐ»Ð¸ Ð‘Ñ‘Ñ€Ñ‡. ",
        "ç¶²ä¸Šç˜‹å‚³ä¸€å¼µç…§ç‰‡ï¼ŒæŒ‡åœ¨æ¸¯éµçš„æ©Ÿå ´å¿«ç¶«åˆ—è»Šæœ‰åºŠè¨è¹¤å½±ã€‚. é€šè¨Šäº‹å‹™ç®¡ç†å±€å…¬å¸ƒï¼Œé¦™æ¸¯å¯¬é » (01310.HK)æ–¼1è‡³4æœˆæœŸé–“ç™¼ç”Ÿä¸‰å®—æ¶‰åŠå›ºå®šå’Œæµå‹•é›»è¨Šæœå‹™ç”¨æˆ¶çš„éŒ¯èª¤æ”¶å¸³äº‹ä»¶ã€‚. ä¸‰å®—äº‹ä»¶å…±å½±éŸ¿è¶…éŽ1è¬åå®¢æˆ¶",
        "í•˜ëŠ”ë° ì¹´ìš´í„°ê°€ ì–´ë””ì— ìžˆì–´ìš”ê†ƒêŽ­ê†ˆêŒ êŠ¨ê¦ê²ê…‰ê†…ê‰šê…‰ê‹ê‚·ê‚¶êŒ Ù„Ø£Ø­ÙŠØ§Ø¡ ØªÙ…Ø§Ø±ÙŠÙ† ØªØªØ·Ù„Ø¨ Ù…Ù† [MASK] [PAD] [CLS][SEP]" "def normalize(text, maxlen=0, isolate_digits=False):     text = unicodedata.normalize('NFC', text)     if maxlen > 0:         if isolate_digits:             regex = '\d|[^\n\d]{,%d}\n{1,100}|[^\n\d]{1,%d}' % (maxlen, maxlen)         else:             regex = '.{,%d}\n{1,100}|.{1,%d}' % (maxlen, maxlen)     else:         if isolate_digits:             regex = '\d|[^\n\d]*\n+|[^\n\d]+'         else:             regex = '.*\n+|.+'     return [t.encode() for t in re.findall(regex, text)]",
        "êƒ›ê¢ê„§êê‡©ê‡­ê£ê„ˆê’œê€‰ê’‰êŠ°êêƒ¢êŒ ê‡©ê¤ê‘­êŠ‚ê§êŽê’œêˆ¿ê…‰ê‡¬ê„‰ê…‡ê„œêŒ ",
        'thead> <tbody> <tr> <td align="left"><a href="https://comparable.limsi.fr/bucc2018/bucc2018-task.html" rel="nofollow">BUCC</a></td> <td align="left"><a href="https://huggingface.co/datasets/mteb/bucc-bitext-mining" rel="nofollow">mteb/bucc-bitext-mining</a></td> <td align="left">BUCC bitext mining dataset</td> <td align="left">BitextMining</td> <td align="left">s2s</td> <td align="right">4</td> <td align="right">0</td> <td align="right">0</td> <td align="right">641684</td> <td align="right">0</td> <td align="right">0</td> <td align="right">101.3</td> </tr> <tr> <td align="left"><a href="https://github.com/facebookresearch/LASER/tree/main/data/tatoeba/v1">Tatoeba</a></td> <td align="left"><a href="https://huggingface.co/datasets/mteb/tatoeba-bitext-mining" rel="nofollow">mteb/tatoeba-bitext-mining</a></td> <td align="left">1,000 English-aligned sentence pairs for each language based on the Tatoeba corpus</td> <td align="left">BitextMining</td> <td align="left">s2s</td> <td align="right">112</td> <td align="right">0</td> <td align="right">0</td> <td align="right">2000</td> <td align="right">0</td> <td align="right">0</td> <td align="right">39.4</td> </tr> <tr> <td align="left"><a href="https://aclanthology.org/W19-6138/" rel="nofollow">Bornholm parallel</a></td> <td align="left"><a href="https://huggingface.co/datasets/strombergnlp/bornholmsk_parallel" rel="nofollow">strombergnlp/bornholmsk_parallel</a></td> <td align="left">Danish Bornholmsk Parallel Corpus.</td> <td align="left">BitextMining</td> <td align="left">s2s</td> <td align="right">2</td> <td align="right">100</td> <td align="right">100</td> <td align="right">100</td> <td align="right">64.6</td> <td align="right">86.2</td> <td align="right">89.7</td> </tr> <tr> <td align="left"><a href="https://arxiv.org/abs/2104.06893" rel="nofollow">AmazonCounterfactualClassification</a></td> <td align="left"><a href="https://huggingface.co/datasets/mteb/amazon_counterfactual" rel="nofollow">mteb/amazon_counterfactual</a></td> <td align="left">A collection of Amazon customer reviews annotated for counterfactual detection pair classification.</td> <td align="left">Classification</td> <td align="left">s2s</td> <td align="right">4</td> <td align="right">4018</td> <td align="right">335</td> <td align="right">670</td> <td align="right">107.3</td> <td align="right">109.2</td> <td align="right">106.1</td> </tr> <tr> <td align="left"><a href="https://dl.acm.org/doi/10.1145/2507157.2507163" rel="nofollow">AmazonPolarityClassification</a></td> <td align="left"><a href="https://huggingface.co/datasets/mteb/amazon_polarity" rel="nofollow">mteb/amazon_polarity</a></td> <td align="left">Amazon Polarity Classification Dataset.</td> <td align="left">Classification</td> <td align="left">s2s</td> <td align="right">1</td> <td align="right">3600000</td> <td align="right">0</td> <td align="right">400000</td> <td align="right">431.6</td> <td align="right">0</td> <td align="right">431.4</td> </tr> <tr> <td align="left"><a href="https://arxiv.org/abs/2010.02573" rel="nofollow">AmazonReviewsClassification</a></td> <td align="left"><a href="https://huggingface.co/datasets/mteb/amazon_reviews_multi" rel="nofollow">mteb/amazon_reviews_multi</a></td> <td align="left">A collection of Amazon reviews specifically designed to aid research in multilingual text classification.</td> <td align="left">Classification</td> <td align="left">s2s</td> <td align="right">6</td> <td align="right">1200000</td> <td align="right">30000</td> <td align="right">30000</td> <td align="right">160.5</td> <td align="right">159.2</td> <td align="right">160.4</td> </tr> <tr> <td align="left"><a href="https://arxiv.org/abs/2003.04807" rel="nofollow">Banking77Classification</a></td> <td align="left"><a href="https://huggingface.co/datasets/mteb/banking77" rel="nofollow">mteb/banking77</a></td> <td align="left">Dataset composed of online banking queries annotated with their corresponding intents.</td> <td align="left">Classification</td> <td align="left">s2s</td> <td align="right">1</td> <td align="right">10003</td> <td align="right">0</td> <td align="right">3080</td> <td align="right">59.5</td> <td align="right">0</td> <td align="right">54.2</td> </tr> <tr> <td align="left"><a href="https://www.aclweb.org/anthology/D18-1404" rel="nofollow">EmotionClassification</a></td> <td align="left"><a href="https://huggingface.co/datasets/mteb/emotion" rel="nofollow">mteb/emotion</a></td> <td align="left">Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper.</td> <td align="left">Classification</td> <td align="left">s2s</td> <td align="right">1</td> <td align="right">16000</td> <td align="right">2000</td> <td align="right">2000</td> <td align="right">96.8</td> <td align="right">95.3</td> <td align="right">96.6</td> </tr> <tr>',
    ]
    demo(doc)


""" 

"""
