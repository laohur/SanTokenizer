import glob
from collections import Counter
import os
import unicodedata
from timeit import repeat
from langer import is_hanzi,split_chars,split_category,strip_accents,split_lanugage,split_punctuation,Langer
from tokenization import BasicTokenizer



if __name__ == "__main__":
    line=''
    for i in range(128):
        try:
            c=chr(i)
            line+=c
        except:
            pass
    line = 'ã¡[à¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-g[ran]d-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)'
    # s=line
    s=split_chars(line)
    s=split_category(line)
    s=strip_accents(line)
    s=split_lanugage(line)
    s=split_punctuation(line)

    print(s)
    for x in line :
        try:
            c=unicodedata.category(x)
            n=unicodedata.name(x)
            print(x,c,n,is_hanzi(x))
        except:
            print(x,c,'err')
            pass
    # ['[', ']', 'viiipays', '-', 'grand', '-', 'blanc', '-', 'eleve', 'Â»', '(', 'ç™½', 'é«˜', 'å¤§', 'å¤', 'åœ‹', ')']
    # a=tokenizer=Langer()
    a=tokenizer=Langer(do_lower_case=False)
    # b=tokenizer=BasicTokenizer()
    b=tokenizer=BasicTokenizer(do_lower_case=False)
    def test():
        doc0 = """ ï¡¿
                â…§é¦–å…ˆ8.88è®¾ç½® stã€‚art_new_word=True å’Œ output=[aÃ§aÃ­]ï¼Œoutput å°±æ˜¯æœ€ç»ˆï¡¿î´°Â‘ no such name"
                çš„è¾“å‡ºà¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°íƒ‘ìŠ¹ ìˆ˜ì†í•´ì•¼pneumonoultramicroscopicsilicovolcanoconiosis"
                í•˜ëŠ”ë° ì¹´ìš´í„°ê°€ ì–´ë””ì— ìˆì–´ìš”ê†ƒê­ê†ˆêŒ êŠ¨ê¦ê²ê…‰ê†…ê‰šê…‰ê‹ê‚·ê‚¶êŒ Ù„Ø£Ø­ÙŠØ§Ø¡ ØªÙ…Ø§Ø±ÙŠÙ† ØªØªØ·Ù„Ø¨ Ù…Ù† [MASK] [PAD] [CLS][SEP]
               est ğ—´‚ğ—¹­ğ˜œ¶ğ—´²ğ—‚§, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par Â« pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹). 
            """

        for i, line in enumerate(doc0.split('\n')):
            line=line.strip()
            # print(tokenizer.tokenize(line))
            print(a.tokenize(line))
            print(b.tokenize(line))
            # a.tokenize(line)
    test()
    s = 'à¸§à¸£à¸£à¸“à¸à¸‡à¸©à¹Œà¹€à¸›à¹‡à¸™à¸™à¸±à¸à¸¨à¸¶à¸à¸©à¸²à¸Šà¸±à¹‰à¸™à¸›à¸µà¸—à¸µà¹ˆà¸«à¸™à¸¶à¹ˆà¸‡ à¹€à¸£à¸µà¸¢à¸™à¸ªà¸²à¸‚à¸²à¸§à¸´à¸—à¸¢à¸²à¸à¸²à¸£à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œà¹à¸¥à¸°à¸ªà¸²à¸£à¸ªà¸™à¹€à¸—à¸¨à¸„à¸“à¸°à¸§à¸´à¸—à¸¢à¸²à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸›à¸£à¸°à¸¢à¸¸à¸à¸•à¹Œà¹à¸¥à¸°à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸­à¸¢à¸¹à¹ˆà¸—à¸µà¹ˆà¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢à¸‚à¸­à¸™à¹à¸à¹ˆà¸™à¸§à¸´à¸—à¸¢à¸²à¹€à¸‚à¸•à¸«à¸™à¸­à¸‡à¸„à¸²à¸¢à¸¢à¸·à¸¡à¸„à¸·à¸™à¸—à¸£à¸±à¸à¸¢à¸²à¸à¸£à¸«à¹‰à¸­à¸‡à¸ªà¸¡à¸¸à¸”à¹€à¸­à¸à¸ªà¸²à¸£à¸ªà¸±à¸¡à¸¡à¸™à¸²à¸„à¸­à¸¡à¸à¸´à¸§à¹€à¸•à¸­à¸£à¹Œà¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œà¸à¸±à¸šà¸à¸²à¸£à¸à¸±à¸’à¸™à¸²à¹€à¸à¸¡à¹à¸¡à¸§à¸à¸´à¸™à¸›à¸¥à¸²à¸«à¸´à¸§à¸§à¸§à¹„à¸«à¸¡à¸«à¸¥à¸±à¸à¸ªà¸¹à¸•à¸£à¹ƒà¸«à¸¡à¹ˆà¸ªà¸”à¸ªà¸”à¸—à¸™à¹„à¸”à¹‰'
    print(a.tokenize(s))
    # import timeit
    # print(timeit.timeit("unicodedata.category('c')",setup="import unicodedata",number=1000000))  # 0.08s
    # print(timeit.timeit("unicodedata.name('c').split(' ')[0]",setup="import unicodedata",number=1000000)) # 0.32s
    # print(timeit.timeit("unicodedata.name('c').split(' ')[0].strip()",setup="import unicodedata",number=1000000)) # 0.36s
    # print(timeit.timeit("test()",setup="from __main__ import test",number=10000)) # 11.02s
    
""" tokenize result (both basic)
Langer
BERT 

[]
[]
['é¦–', 'å…ˆ', '8', '.', '88', 'è®¾', 'ç½®', 'st', 'ã€‚', 'art', '_', 'new', '_', 'word', '=', 'true', 'å’Œ', 'output', '=', '[', 'acai', ']', 'ï¼Œ', 'output', 'å°±', 'æ˜¯', 'æœ€', 'ç»ˆ', 'no', 'such', 'name', '"']
['é¦–', 'å…ˆ', '8', '.', '88', 'è®¾', 'ç½®', 'st', 'ã€‚', 'art', '_', 'new', '_', 'word', '=', 'true', 'å’Œ', 'output', '=', '[', 'acai', ']', 'ï¼Œ', 'output', 'å°±', 'æ˜¯', 'æœ€', 'ç»ˆ', 'no', 'such', 'name', '"']
['çš„', 'è¾“', 'å‡º', 'à¸„', 'à¸“à¸ˆà¸°à¸ˆ', 'à¸”à¸', 'à¸˜', 'à¹à¸•', 'à¸‡à¸‡à¸²à¸™à¹€à¸¡', 'à¸­à¹„à¸£à¸„à¸°', 'á„á…¡á†¸', 'á„‰á…³á†¼', 'á„‰á…®', 'á„‰á…©á†¨', 'á„’á…¢', 'á„‹á…£', 'pneumonoultramicroscopicsilicovolcanoconiosis', '"']
['çš„', 'è¾“', 'å‡º', 'à¸„à¸“à¸ˆà¸°à¸ˆà¸”à¸à¸˜à¹à¸•à¸‡à¸‡à¸²à¸™à¹€à¸¡à¸­à¹„à¸£à¸„à¸°á„á…¡á†¸á„‰á…³á†¼', 'á„‰á…®á„‰á…©á†¨á„’á…¢á„‹á…£pneumonoultramicroscopicsilicovolcanoconiosis', '"']
['á„’á…¡', 'á„‚á…³á†«', 'á„ƒá…¦', 'á„á…¡', 'á„‹á…®á†«', 'á„á…¥', 'á„€á…¡', 'á„‹á…¥', 'á„ƒá…µ', 'á„‹á…¦', 'á„‹á…µá†»', 'á„‹á…¥', 'á„‹á…­', 'ê†ƒ', 'ê­', 'ê†ˆ', 'êŒ ', 'êŠ¨', 'ê¦', 'ê²', 'ê…‰', 'ê†…', 'ê‰š', 'ê…‰', 'ê‹', 'ê‚·', 'ê‚¶', 'êŒ 
', 'Ù„Ø§Ø­ÙŠØ§Ø¡', 'ØªÙ…Ø§Ø±ÙŠÙ†', 'ØªØªØ·Ù„Ø¨', 'Ù…Ù†', '[MASK]', '[PAD]', '[', 'cls', ']', '[', 'sep', ']']
['á„’á…¡á„‚á…³á†«á„ƒá…¦', 'á„á…¡á„‹á…®á†«á„á…¥á„€á…¡', 'á„‹á…¥á„ƒá…µá„‹á…¦', 'á„‹á…µá†»á„‹á…¥á„‹á…­ê†ƒê­ê†ˆêŒ êŠ¨ê¦ê²ê…‰ê†…ê‰šê…‰ê‹ê‚·ê‚¶êŒ Ù„Ø§Ø­ÙŠØ§Ø¡', 'ØªÙ…Ø§Ø±ÙŠÙ†', 'ØªØªØ·Ù„Ø¨', 'Ù…Ù†', '[MASK]', '[PAD]', '[', 'cls', ']', '[', 'sep', ']']
['est', ',', 'ou', '"', 'phiow', '-', 'bjij', '-', 'lhjij', '-', 'lhjij', '"', ',', 'ce', 'que', 'l', "'", 'on', 'peut', 'traduire', 'par', 'Â«', 'pays', '-', 'grand', '-', 'blanc', '-', 'eleve', 'Â»', '(', 'ç™½', 'é«˜', 'å¤§', 'å¤', 'åœ‹', ')', '.']
['est', 'ğ—´‚ğ—¹­ğ˜œ¶ğ—´²ğ—‚§', ',', 'ou', '"', 'phiow', '-', 'bjij', '-', 'lhjij', '-', 'lhjij', '"', ',', 'ce', 'que', 'l', "'", 'on', 'peut', 'traduire', 'par', 'Â«', 'pays', '-', 'grand', '-', 'blanc', '-', 'eleve', 'Â»', '(', 'ç™½', 'é«˜', 'å¤§', 'å¤', 'åœ‹', ')', '.']
[]
[]
"""
