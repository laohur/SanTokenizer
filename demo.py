import glob
from collections import Counter
import os
import unicodedata
from timeit import repeat
from langer import is_hanzi,get_block,shave_marks,tokenize


if __name__ == "__main__":
    line = 'pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)'

    #  get unicode block of a character
    for x in line:
        is_chinese=is_hanzi(x)
        start,end,block_name=get_block(x)
        char_name=unicodedata.name(x)
    
    # convert varirants to base character
    print(shave_marks(line))
    # pays-grand-blanc-eleve Â» (ç™½é«˜å¤§å¤åœ‹)

    # token a sentence for all languages
    print(tokenize(line))
    # ['pays', '-', 'grand', '-', 'blanc', '-', 'Ã©levÃ©', 'Â»', '(', 'ç™½', 'é«˜', 'å¤§', 'å¤', 'åœ‹', ')']

    def test():
        doc0 = """
                é¦–å…ˆ8.88è®¾ç½® stã€‚art_new_word=True å’Œ output=[aÃ§aÃ­]ï¼Œoutput å°±æ˜¯æœ€ç»ˆï¡¿î´°Â‘ no such name"
                çš„è¾“å‡ºà¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°íƒ‘ìŠ¹ ìˆ˜ì†í•´ì•¼pneumonoultramicroscopicsilicovolcanoconiosis"
                í•˜ëŠ”ë° ì¹´ìš´í„°ê°€ ì–´ë””ì— ìˆì–´ìš”ê†ƒê­ê†ˆêŒ êŠ¨ê¦ê²ê…‰ê†…ê‰šê…‰ê‹ê‚·ê‚¶êŒ Ù„Ø£Ø­ÙŠØ§Ø¡ ØªÙ…Ø§Ø±ÙŠÙ† ØªØªØ·Ù„Ø¨ Ù…Ù† 
               est ğ—´‚ğ—¹­ğ˜œ¶ğ—´²ğ—‚§, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par Â« pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹). 
            """

        for i, line in enumerate(doc0.split('\n')):
            line=line.strip()
            # print(tokenize(line))
            tokenize(line)
    test()
    import timeit
    print(timeit.timeit("unicodedata.category('c')",setup="import unicodedata",number=1000000))  # 0.11s
    print(timeit.timeit("unicodedata.name('c').split(' ')[0]",setup="import unicodedata",number=1000000)) # 0.46s
    print(timeit.timeit("unicodedata.name('c').split(' ')[0].strip()",setup="import unicodedata",number=1000000)) # 0.47s
    print(timeit.timeit("test()",setup="from __main__ import test",number=10000)) # 7.60s
    