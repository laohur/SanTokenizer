import glob
from collections import Counter
import os
import unicodedata
from timeit import repeat
from langer import BasicTokenizer

if __name__ == "__main__":

    line = '[à¸±à¸µà¸´à¹Œà¸·à¹‡à¹à¸¶]â…§pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹)'
    tokenizer=BasicTokenizer()
    print(tokenizer.tokenize(line))

    # ['[', ']', 'viiipays', '-', 'grand', '-', 'blanc', '-', 'eleve', 'Â»', '(', 'ç™½', 'é«˜', 'å¤§', 'å¤', 'åœ‹', ')']

    def test():
        doc0 = """
                é¦–å…ˆ8.88è®¾ç½® stã€‚art_new_word=True å’Œ output=[aÃ§aÃ­]ï¼Œoutput å°±æ˜¯æœ€ç»ˆï¡¿î´°Â‘ no such name"
                çš„è¾“å‡ºà¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°íƒ‘ìŠ¹ ìˆ˜ì†í•´ì•¼pneumonoultramicroscopicsilicovolcanoconiosis"
                í•˜ëŠ”ë° ì¹´ìš´í„°ê°€ ì–´ë””ì— ìˆì–´ìš”ê†ƒê­ê†ˆêŒ êŠ¨ê¦ê²ê…‰ê†…ê‰šê…‰ê‹ê‚·ê‚¶êŒ Ù„Ø£Ø­ÙŠØ§Ø¡ ØªÙ…Ø§Ø±ÙŠÙ† ØªØªØ·Ù„Ø¨ Ù…Ù† 
               est ğ—´‚ğ—¹­ğ˜œ¶ğ—´²ğ—‚§, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par Â« pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹). 
            """

        for i, line in enumerate(doc0.split('\n')):
            line=line.strip()
            print(tokenizer.tokenize(line))
            tokenizer.tokenize(line)
    test()

    import timeit
    # print(timeit.timeit("unicodedata.category('c')",setup="import unicodedata",number=1000000))  # 0.11s
    # print(timeit.timeit("unicodedata.name('c').split(' ')[0]",setup="import unicodedata",number=1000000)) # 0.46s
    # print(timeit.timeit("unicodedata.name('c').split(' ')[0].strip()",setup="import unicodedata",number=1000000)) # 0.47s
    # print(timeit.timeit("test()",setup="from __main__ import test",number=10000)) # 7.60s
    