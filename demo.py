import glob
from collections import Counter
import os
import unicodedata
from timeit import repeat
from langer import is_hanzi,split_chars,split_category,strip_accents,split_lanugage,split_punctuation,Langer
from tokenization import BasicTokenizer



if __name__ == "__main__":
    line=''
    for i in range(128):
        try:
            c=chr(i)
            line+=c
        except:
            pass
    line = 'ใก[เธเธธเธเธเธฐเธเธฑเธเธเธดเธเธตเนเธเนเธเธเธฒเธเนเธกเธทเนเธญเนเธฃเธเธฐเธฑเธตเธดเนเธทเนเนเธถ]โงpays-g[ran]d-blanc-รฉlevรฉ ยป (็ฝ้ซๅคงๅคๅ)'
    # s=line
    s=split_chars(line)
    s=split_category(line)
    s=strip_accents(line)
    s=split_lanugage(line)
    s=split_punctuation(line)

    print(s)
    for x in line :
        try:
            c=unicodedata.category(x)
            n=unicodedata.name(x)
            print(x,c,n,is_hanzi(x))
        except:
            print(x,c,'err')
            pass
    # ['[', ']', 'viiipays', '-', 'grand', '-', 'blanc', '-', 'eleve', 'ยป', '(', '็ฝ', '้ซ', 'ๅคง', 'ๅค', 'ๅ', ')']
    # a=tokenizer=Langer()
    a=tokenizer=Langer(do_lower_case=False)
    # b=tokenizer=BasicTokenizer()
    b=tokenizer=BasicTokenizer(do_lower_case=False)
    def test():
        doc0 = """ ๏กฟ
                โง้ฆๅ8.88่ฎพ็ฝฎ stใart_new_word=True ๅ output=[aรงaรญ]๏ผoutput ๅฐฑๆฏๆ็ป๏กฟ๎ดฐย no such name"
                ็่พๅบเธเธธเธเธเธฐเธเธฑเธเธเธดเธเธตเนเธเนเธเธเธฒเธเนเธกเธทเนเธญเนเธฃเธเธฐํ์น ์์ํด์ผpneumonoultramicroscopicsilicovolcanoconiosis"
                ํ๋๋ฐ ์นด์ดํฐ๊ฐ ์ด๋์ ์์ด์๊๊ญ๊๊๊จ๊ฆ๊ฒ๊๊๊๊๊๊ท๊ถ๊ูุฃุญูุงุก ุชูุงุฑูู ุชุชุทูุจ ูู [MASK] [PAD] [CLS][SEP]
               est ๐ด๐นญ๐ถ๐ดฒ๐ง, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par ยซ pays-grand-blanc-รฉlevรฉ ยป (็ฝ้ซๅคงๅคๅ). 
            """

        for i, line in enumerate(doc0.split('\n')):
            line=line.strip()
            # print(tokenizer.tokenize(line))
            print(a.tokenize(line))
            print(b.tokenize(line))
            # a.tokenize(line)
    test()
    s = 'เธงเธฃเธฃเธเธเธเธฉเนเนเธเนเธเธเธฑเธเธจเธถเธเธฉเธฒเธเธฑเนเธเธเธตเธเธตเนเธซเธเธถเนเธ เนเธฃเธตเธขเธเธชเธฒเธเธฒเธงเธดเธเธขเธฒเธเธฒเธฃเธเธญเธกเธเธดเธงเนเธเธญเธฃเนเนเธฅเธฐเธชเธฒเธฃเธชเธเนเธเธจเธเธเธฐเธงเธดเธเธขเธฒเธจเธฒเธชเธเธฃเนเธเธฃเธฐเธขเธธเธเธเนเนเธฅเธฐเธงเธดเธจเธงเธเธฃเธฃเธกเธจเธฒเธชเธเธฃเนเธญเธขเธนเนเธเธตเนเธกเธซเธฒเธงเธดเธเธขเธฒเธฅเธฑเธขเธเธญเธเนเธเนเธเธงเธดเธเธขเธฒเนเธเธเธซเธเธญเธเธเธฒเธขเธขเธทเธกเธเธทเธเธเธฃเธฑเธเธขเธฒเธเธฃเธซเนเธญเธเธชเธกเธธเธเนเธญเธเธชเธฒเธฃเธชเธฑเธกเธกเธเธฒเธเธญเธกเธเธดเธงเนเธเธญเธฃเนเธเธฑเธเธเธฒเธเธฃเธฐเธเธดเธฉเธเนเธเธฑเธเธเธฒเธฃเธเธฑเธเธเธฒเนเธเธกเนเธกเธงเธเธดเธเธเธฅเธฒเธซเธดเธงเธงเธงเนเธซเธกเธซเธฅเธฑเธเธชเธนเธเธฃเนเธซเธกเนเธชเธเธชเธเธเธเนเธเน'
    s='เบชเบปเบกเปเบเบฑเบเบเบฐเปเบเบปเปเบฒเบขเบนเปเบซเบปเบงเบเปเบฃเบปเบกเปเบเบเบเบปเบเบเบณเบเบธเบเบณเบฅเบธเบเบเปเบฒเบเปเบกเบทเบญเบเปเบฅเบฐเบเบฐเบชเบฒเบเบชเบฐเปเบฒเบเบปเบเบเปเบฒเบงเปเบเปเบงเปเบฒเบเบธเบเบชเบตเบญเบฐเบเบธเบเบฐเบขเบฒเปเบเบชเบฐเปเปเบเบฐเบญเบปเบเบเบฑเปเบเปเบเบฑเบเบเบธเบเบเบตเปเบเปเบฒเบเปเบกเบทเบญเบเบเบต เบกเบตเบเบธเบเบเบฒเบเบเบปเบเบชเบณเบเบฑเบเบเบตเปเปเบเบตเบเปเบเปเบเปเบงเบฅเบฒเบเปเปเบกเบฒ เปเบเบฅเบฒเบเบฐเบเบฒเบเบเบญเบเบเบฐเบญเบปเบเบซเบผเบฒเบเบเบปเบ เปเบเบฑเปเบ เบชเบปเบกเปเบเบฑเบเบเบฐเปเบเบปเปเบฒเบเบธเบเบเบปเบเบเบธเบฅเบต, เบเบฐเบเบฒเบเบชเบปเบกเปเบเบฑเบเบเบฐเบเบธเบเบเบฐเบเบญเบเบเปเบฒเบเบธเบฅเบฒเปเบฅเบเบกเบฐเบซเบฒเบฅเบฒเบ เปเบเบฑเบเบเบปเปเบ เปเบเบเบฒเบเบเปเบฒเบเบงเบฑเบเบเบฐเบเบฐเบเบตเบเปเบกเบตเบเบฐเบงเบตเบเบปเบเบชเบณเบเบฑเบ เปเบเบฑเปเบ เปเบเบปเปเบฒเบเปเบฒเบเบณเบกเบฒเบเบดเปเบเบเปเบเบเบฐเปเบเบเบชเบธเบฅเบดเบเบฐเบงเบปเบ เบเบปเบกเบกเบฐเบเบธเบเปเบชเบเบฒเบเบดเบเบฑเบ เบซเบผเบทเปเบเบปเปเบฒเบเปเบฒเบเบธเปเบ เปเบเบดเปเบเปเบเบฑเบเบเบฐเปเบญเบฅเบปเบ เปเบเบฑเบเบเบปเปเบ'
    print(a.tokenize(s))
    # import timeit
    # print(timeit.timeit("unicodedata.category('c')",setup="import unicodedata",number=1000000))  # 0.08s
    # print(timeit.timeit("unicodedata.name('c').split(' ')[0]",setup="import unicodedata",number=1000000)) # 0.32s
    # print(timeit.timeit("unicodedata.name('c').split(' ')[0].strip()",setup="import unicodedata",number=1000000)) # 0.36s
    # print(timeit.timeit("test()",setup="from __main__ import test",number=10000)) # 11.02s
    
""" tokenize result (both basic)
Langer
BERT 

[]
[]
['้ฆ', 'ๅ', '8', '.', '88', '่ฎพ', '็ฝฎ', 'st', 'ใ', 'art', '_', 'new', '_', 'word', '=', 'true', 'ๅ', 'output', '=', '[', 'acai', ']', '๏ผ', 'output', 'ๅฐฑ', 'ๆฏ', 'ๆ', '็ป', 'no', 'such', 'name', '"']
['้ฆ', 'ๅ', '8', '.', '88', '่ฎพ', '็ฝฎ', 'st', 'ใ', 'art', '_', 'new', '_', 'word', '=', 'true', 'ๅ', 'output', '=', '[', 'acai', ']', '๏ผ', 'output', 'ๅฐฑ', 'ๆฏ', 'ๆ', '็ป', 'no', 'such', 'name', '"']
['็', '่พ', 'ๅบ', 'เธ', 'เธเธเธฐเธ', 'เธเธ', 'เธ', 'เนเธ', 'เธเธเธฒเธเนเธก', 'เธญเนเธฃเธเธฐ', 'แแกแธ', 'แแณแผ', 'แแฎ', 'แแฉแจ', 'แแข', 'แแฃ', 'pneumonoultramicroscopicsilicovolcanoconiosis', '"']
['็', '่พ', 'ๅบ', 'เธเธเธเธฐเธเธเธเธเนเธเธเธเธฒเธเนเธกเธญเนเธฃเธเธฐแแกแธแแณแผ', 'แแฎแแฉแจแแขแแฃpneumonoultramicroscopicsilicovolcanoconiosis', '"']
['แแก', 'แแณแซ', 'แแฆ', 'แแก', 'แแฎแซ', 'แแฅ', 'แแก', 'แแฅ', 'แแต', 'แแฆ', 'แแตแป', 'แแฅ', 'แแญ', '๊', '๊ญ', '๊', '๊', '๊จ', '๊ฆ', '๊ฒ', '๊', '๊', '๊', '๊', '๊', '๊ท', '๊ถ', '๊
', 'ูุงุญูุงุก', 'ุชูุงุฑูู', 'ุชุชุทูุจ', 'ูู', '[MASK]', '[PAD]', '[', 'cls', ']', '[', 'sep', ']']
['แแกแแณแซแแฆ', 'แแกแแฎแซแแฅแแก', 'แแฅแแตแแฆ', 'แแตแปแแฅแแญ๊๊ญ๊๊๊จ๊ฆ๊ฒ๊๊๊๊๊๊ท๊ถ๊ูุงุญูุงุก', 'ุชูุงุฑูู', 'ุชุชุทูุจ', 'ูู', '[MASK]', '[PAD]', '[', 'cls', ']', '[', 'sep', ']']
['est', ',', 'ou', '"', 'phiow', '-', 'bjij', '-', 'lhjij', '-', 'lhjij', '"', ',', 'ce', 'que', 'l', "'", 'on', 'peut', 'traduire', 'par', 'ยซ', 'pays', '-', 'grand', '-', 'blanc', '-', 'eleve', 'ยป', '(', '็ฝ', '้ซ', 'ๅคง', 'ๅค', 'ๅ', ')', '.']
['est', '๐ด๐นญ๐ถ๐ดฒ๐ง', ',', 'ou', '"', 'phiow', '-', 'bjij', '-', 'lhjij', '-', 'lhjij', '"', ',', 'ce', 'que', 'l', "'", 'on', 'peut', 'traduire', 'par', 'ยซ', 'pays', '-', 'grand', '-', 'blanc', '-', 'eleve', 'ยป', '(', '็ฝ', '้ซ', 'ๅคง', 'ๅค', 'ๅ', ')', '.']
[]
[]
"""
